{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPAhXVbs3FgUB3BuTEB8Hfi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mohan24047/LIVE-AI/blob/main/NLP_WordEmbeddings_CNN_student_version.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "E2uTYMY0T49Z"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dict(words):\n",
        "    \"\"\"\n",
        "    Generate word-to-index and index-to-word dictionaries.\n",
        "\n",
        "    Args:\n",
        "        words (list of str): List of words from a tokenized corpus.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (word2Ind, Ind2word) dictionaries.\n",
        "    \"\"\"\n",
        "    unique_words = sorted(set(words))  # Sort for consistency\n",
        "    word2Ind = {word: i for i, word in enumerate(unique_words)}\n",
        "    Ind2word = {i: word for word, i in word2Ind.items()}\n",
        "\n",
        "    return word2Ind, Ind2word\n",
        "\n",
        "# Example usage\n",
        "words = [\"hello\", \"world\", \"hello\", \"machine\", \"learning\"]\n",
        "word2Ind, Ind2word = get_dict(words)\n",
        "\n",
        "print(\"word2Ind:\", word2Ind)\n",
        "print(\"Ind2word:\", Ind2word)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PfMaoAcMUKHo",
        "outputId": "cdc32364-e996-482c-c2ae-b474a126da26"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "word2Ind: {'hello': 0, 'learning': 1, 'machine': 2, 'world': 3}\n",
            "Ind2word: {0: 'hello', 1: 'learning', 2: 'machine', 3: 'world'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the tokenized version of the corpus\n",
        "words = ['i', 'am', 'happy', 'because', 'i', 'am', 'learning']\n",
        "\n",
        "# Define V. Remember this is the size of the vocabulary\n",
        "V = 5\n",
        "\n",
        "# Get 'word2Ind' and 'Ind2word' dictionaries for the tokenized corpus\n",
        "word2Ind, Ind2word = get_dict(words)\n",
        "\n",
        "\n",
        "# Define first matrix of weights\n",
        "W1 = np.array([[ 0.41687358,  0.08854191, -0.23495225,  0.28320538,  0.41800106],\n",
        "               [ 0.32735501,  0.22795148, -0.23951958,  0.4117634 , -0.23924344],\n",
        "               [ 0.26637602, -0.23846886, -0.37770863, -0.11399446,  0.34008124]])\n",
        "\n",
        "# Define second matrix of weights\n",
        "W2 = np.array([[-0.22182064, -0.43008631,  0.13310965],\n",
        "               [ 0.08476603,  0.08123194,  0.1772054 ],\n",
        "               [ 0.1871551 , -0.06107263, -0.1790735 ],\n",
        "               [ 0.07055222, -0.02015138,  0.36107434],\n",
        "               [ 0.33480474, -0.39423389, -0.43959196]])\n",
        "\n",
        "# Define first vector of biases\n",
        "b1 = np.array([[ 0.09688219],\n",
        "               [ 0.29239497],\n",
        "               [-0.27364426]])\n",
        "\n",
        "# Define second vector of biases\n",
        "b2 = np.array([[ 0.0352008 ],\n",
        "               [-0.36393384],\n",
        "               [-0.12775555],\n",
        "               [-0.34802326],\n",
        "               [-0.07017815]])"
      ],
      "metadata": {
        "id": "hJ1bCBZZUWyz"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print W1\n",
        "W1\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-gD6hw9UaEk",
        "outputId": "77e7fe7e-612c-401e-cde2-9800510b19d7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.41687358,  0.08854191, -0.23495225,  0.28320538,  0.41800106],\n",
              "       [ 0.32735501,  0.22795148, -0.23951958,  0.4117634 , -0.23924344],\n",
              "       [ 0.26637602, -0.23846886, -0.37770863, -0.11399446,  0.34008124]])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print corresponding word for each index within vocabulary's range\n",
        "for i in range(V):\n",
        "    print(Ind2word[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yDOVsHlfUeuC",
        "outputId": "34caaa1d-81b5-4a8b-9e27-6a77838fb186"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "am\n",
            "because\n",
            "happy\n",
            "i\n",
            "learning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loop through each word of the vocabulary\n",
        "for word in word2Ind:\n",
        "    # Extract the column corresponding to the index of the word in the vocabulary\n",
        "    word_embedding_vector = W1[:, word2Ind[word]]\n",
        "    # Print word alongside word embedding vector\n",
        "    print(f'{word}: {word_embedding_vector}')\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wSlbFEAwUiSa",
        "outputId": "019e4d79-dd93-4aff-b3a0-a6408b0ca435"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "am: [0.41687358 0.32735501 0.26637602]\n",
            "because: [ 0.08854191  0.22795148 -0.23846886]\n",
            "happy: [-0.23495225 -0.23951958 -0.37770863]\n",
            "i: [ 0.28320538  0.4117634  -0.11399446]\n",
            "learning: [ 0.41800106 -0.23924344  0.34008124]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Print transposed W2\n",
        "W2.T"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qARikCO0UkkD",
        "outputId": "17340271-c67b-4c30-b8e2-9ab199243b76"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.22182064,  0.08476603,  0.1871551 ,  0.07055222,  0.33480474],\n",
              "       [-0.43008631,  0.08123194, -0.06107263, -0.02015138, -0.39423389],\n",
              "       [ 0.13310965,  0.1772054 , -0.1790735 ,  0.36107434, -0.43959196]])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loop through each word of the vocabulary\n",
        "for word in word2Ind:\n",
        "    # Extract the column corresponding to the index of the word in the vocabulary\n",
        "    word_embedding_vector = W2.T[:, word2Ind[word]]\n",
        "    # Print word alongside word embedding vector\n",
        "    print(f'{word}: {word_embedding_vector}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kPRFaxO4UnLq",
        "outputId": "26892c03-422c-4df8-f3e5-de5157b6318c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "am: [-0.22182064 -0.43008631  0.13310965]\n",
            "because: [0.08476603 0.08123194 0.1772054 ]\n",
            "happy: [ 0.1871551  -0.06107263 -0.1790735 ]\n",
            "i: [ 0.07055222 -0.02015138  0.36107434]\n",
            "learning: [ 0.33480474 -0.39423389 -0.43959196]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute W3 as the average of W1 and W2 transposed\n",
        "W3 = (W1+W2.T)/2\n",
        "\n",
        "# Print W3\n",
        "W3\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4rYzap_Upb0",
        "outputId": "d88103c0-0743-4209-b917-5023b7dbc6dc"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.09752647,  0.08665397, -0.02389858,  0.1768788 ,  0.3764029 ],\n",
              "       [-0.05136565,  0.15459171, -0.15029611,  0.19580601, -0.31673866],\n",
              "       [ 0.19974284, -0.03063173, -0.27839106,  0.12353994, -0.04975536]])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loop through each word of the vocabulary\n",
        "for word in word2Ind:\n",
        "    # Extract the column corresponding to the index of the word in the vocabulary\n",
        "    word_embedding_vector = W3[:, word2Ind[word]]\n",
        "    # Print word alongside word embedding vector\n",
        "    print(f'{word}: {word_embedding_vector}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BNsgOyy4Ur_q",
        "outputId": "ca547888-8cb2-4002-aba5-0a57bec512cf"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "am: [ 0.09752647 -0.05136565  0.19974284]\n",
            "because: [ 0.08665397  0.15459171 -0.03063173]\n",
            "happy: [-0.02389858 -0.15029611 -0.27839106]\n",
            "i: [0.1768788  0.19580601 0.12353994]\n",
            "learning: [ 0.3764029  -0.31673866 -0.04975536]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define the size of the word embedding vectors and save it in the variable 'N'\n",
        "N = 3\n",
        "\n",
        "# Define V. Remember this was the size of the vocabulary in the previous lecture notebooks\n",
        "V = 5"
      ],
      "metadata": {
        "id": "2nWkfiNJUuXW"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define first matrix of weights\n",
        "W1 = np.array([[ 0.41687358,  0.08854191, -0.23495225,  0.28320538,  0.41800106],\n",
        "               [ 0.32735501,  0.22795148, -0.23951958,  0.4117634 , -0.23924344],\n",
        "               [ 0.26637602, -0.23846886, -0.37770863, -0.11399446,  0.34008124]])\n",
        "\n",
        "# Define second matrix of weights\n",
        "W2 = np.array([[-0.22182064, -0.43008631,  0.13310965],\n",
        "               [ 0.08476603,  0.08123194,  0.1772054 ],\n",
        "               [ 0.1871551 , -0.06107263, -0.1790735 ],\n",
        "               [ 0.07055222, -0.02015138,  0.36107434],\n",
        "               [ 0.33480474, -0.39423389, -0.43959196]])\n",
        "\n",
        "# Define first vector of biases\n",
        "b1 = np.array([[ 0.09688219],\n",
        "               [ 0.29239497],\n",
        "               [-0.27364426]])\n",
        "\n",
        "# Define second vector of biases\n",
        "b2 = np.array([[ 0.0352008 ],\n",
        "               [-0.36393384],\n",
        "               [-0.12775555],\n",
        "               [-0.34802326],\n",
        "               [-0.07017815]])"
      ],
      "metadata": {
        "id": "L-nh7Q9kUxYN"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(f'V (vocabulary size): {V}')\n",
        "print(f'N (embedding size / size of the hidden layer): {N}')\n",
        "print(f'size of W1: {W1.shape} (NxV)')\n",
        "print(f'size of b1: {b1.shape} (Nx1)')\n",
        "print(f'size of W2: {W2.shape} (VxN)')\n",
        "print(f'size of b2: {b2.shape} (Vx1)')\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aTM4QYp3U0ao",
        "outputId": "327272cc-723f-44fb-9071-4481717b8319"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "V (vocabulary size): 5\n",
            "N (embedding size / size of the hidden layer): 3\n",
            "size of W1: (3, 5) (NxV)\n",
            "size of b1: (3, 1) (Nx1)\n",
            "size of W2: (5, 3) (VxN)\n",
            "size of b2: (5, 1) (Vx1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the tokenized version of the corpus\n",
        "words = ['i', 'am', 'happy', 'because', 'i', 'am', 'learning']\n",
        "\n",
        "# Get 'word2Ind' and 'Ind2word' dictionaries for the tokenized corpus\n",
        "word2Ind, Ind2word = get_dict(words)\n",
        "\n",
        "# Define the 'get_windows' function as seen in a previous notebook\n",
        "def get_windows(words, C):\n",
        "    i = C\n",
        "    while i < len(words) - C:\n",
        "        center_word = words[i]\n",
        "        context_words = words[(i - C):i] + words[(i+1):(i+C+1)]\n",
        "        yield context_words, center_word\n",
        "        i += 1\n",
        "\n",
        "# Define the 'word_to_one_hot_vector' function as seen in a previous notebook\n",
        "def word_to_one_hot_vector(word, word2Ind, V):\n",
        "    one_hot_vector = np.zeros(V)\n",
        "    one_hot_vector[word2Ind[word]] = 1\n",
        "    return one_hot_vector\n",
        "\n",
        "# Define the 'context_words_to_vector' function as seen in a previous notebook\n",
        "def context_words_to_vector(context_words, word2Ind, V):\n",
        "    context_words_vectors = [word_to_one_hot_vector(w, word2Ind, V) for w in context_words]\n",
        "    context_words_vectors = np.mean(context_words_vectors, axis=0)\n",
        "    return context_words_vectors\n",
        "\n",
        "# Define the generator function 'get_training_example' as seen in a previous notebook\n",
        "def get_training_example(words, C, word2Ind, V):\n",
        "    for context_words, center_word in get_windows(words, C):\n",
        "        yield context_words_to_vector(context_words, word2Ind, V), word_to_one_hot_vector(center_word, word2Ind, V)\n",
        ""
      ],
      "metadata": {
        "id": "ovATrhGwU3Vl"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save generator object in the 'training_examples' variable with the desired arguments\n",
        "training_examples = get_training_example(words, 2, word2Ind, V)\n",
        ""
      ],
      "metadata": {
        "id": "1Ozdb1fzU7oj"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get first values from generator\n",
        "x_array, y_array = next(training_examples)"
      ],
      "metadata": {
        "id": "nZrnL7rtU_IY"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print context words vector\n",
        "x_array\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "axHA9ue-VC8V",
        "outputId": "64586110-f5bc-4c10-8766-197cd024d2f6"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.25, 0.25, 0.  , 0.5 , 0.  ])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print one hot vector of center word\n",
        "y_array"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oyyX73DOVEBI",
        "outputId": "843dd626-1a99-49b5-f445-3e9322ed1974"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 1., 0., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy vector\n",
        "x = x_array.copy()\n",
        "\n",
        "# Reshape it\n",
        "x.shape = (V, 1)\n",
        "\n",
        "# Print it\n",
        "print(f'x:\\n{x}\\n')\n",
        "\n",
        "# Copy vector\n",
        "y = y_array.copy()\n",
        "\n",
        "# Reshape it\n",
        "y.shape = (V, 1)\n",
        "\n",
        "# Print it\n",
        "print(f'y:\\n{y}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fd_9i2Y3VGRd",
        "outputId": "faf94439-0b59-44af-dd40-2a586c5962c9"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x:\n",
            "[[0.25]\n",
            " [0.25]\n",
            " [0.  ]\n",
            " [0.5 ]\n",
            " [0.  ]]\n",
            "\n",
            "y:\n",
            "[[0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the ReLU function\n",
        "def relu(z):\n",
        "    return np.maximum(0, z)\n",
        "\n",
        "# Define the Softmax function\n",
        "def softmax(z):\n",
        "    exp_z = np.exp(z - np.max(z))  # Subtract max for numerical stability\n",
        "    return exp_z / np.sum(exp_z)\n"
      ],
      "metadata": {
        "id": "P8jDOgP1VM5R"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "source": [
        "import numpy as np\n",
        "\n",
        "# Example: Assuming W1, x, and b1 are defined\n",
        "z1 = np.dot(W1, x) + b1  # Compute linear transformation before activation"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "wku8UB_9WEvE"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print z1\n",
        "z1\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5s642snZWHdy",
        "outputId": "36d3d89f-060b-40ee-972d-d9552bbfe9ac"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.36483875],\n",
              "       [ 0.63710329],\n",
              "       [-0.3236647 ]])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Compute h (z1 after applying ReLU function)\n",
        "h = relu(z1)\n",
        "\n",
        "# Print h\n",
        "h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TVDpnoxhWK_6",
        "outputId": "15c27ad9-a879-4589-d6c0-13ffe29b0c8e"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.36483875],\n",
              "       [0.63710329],\n",
              "       [0.        ]])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute z2 (values before applying softmax)\n",
        "z2 = np.dot(W2, h) + b2\n",
        "\n",
        "# Print z2\n",
        "print(z2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ipBCo4MWN8o",
        "outputId": "15dffec1-5932-40b6-c231-7d4eec6771e9"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-0.31973737]\n",
            " [-0.28125477]\n",
            " [-0.09838369]\n",
            " [-0.33512159]\n",
            " [-0.19919612]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute y_hat (probabilities after applying softmax)\n",
        "y_hat = softmax(z2)\n",
        "\n",
        "# Print y_hat\n",
        "print(y_hat)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4rKKsQjWZ_Z",
        "outputId": "ef59b3ce-67a2-41b5-c142-df262b553285"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.18519074]\n",
            " [0.19245626]\n",
            " [0.23107446]\n",
            " [0.18236353]\n",
            " [0.20891502]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print prediction\n",
        "y_hat"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FyppU9liWj9N",
        "outputId": "772b4364-fb04-4e11-b5c7-8d68a339f1d2"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.18519074],\n",
              "       [0.19245626],\n",
              "       [0.23107446],\n",
              "       [0.18236353],\n",
              "       [0.20891502]])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print target value\n",
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7vENMGYSWmEx",
        "outputId": "aa4151e8-bb11-4331-db4f-c35c282f24ce"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_entropy_loss(y_predicted, y_actual):\n",
        "    # Fill the loss variable with your code\n",
        "    loss = None\n",
        "    return loss"
      ],
      "metadata": {
        "id": "fT_xDxytWpcF"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print value of cross entropy loss for prediction and target value\n",
        "cross_entropy_loss(y_hat, y)"
      ],
      "metadata": {
        "id": "Pkacz0Z5WsCb"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute gradient of loss with respect to b2\n",
        "grad_b2 = y_hat - y\n",
        "\n",
        "# Print the gradient\n",
        "print(grad_b2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qq5fB9k3XIrU",
        "outputId": "b4baeda2-b106-41f8-cd3f-0a24f94c82d0"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0.18519074]\n",
            " [ 0.19245626]\n",
            " [-0.76892554]\n",
            " [ 0.18236353]\n",
            " [ 0.20891502]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute gradient of loss with respect to W2\n",
        "grad_W2 = np.dot((y_hat - y), h.T)\n",
        "\n",
        "# Print the gradient matrix\n",
        "print(grad_W2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jiIL0lyvXNip",
        "outputId": "a60ed369-ada3-44b9-a3ed-a22e4f599a46"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0.06756476  0.11798563  0.        ]\n",
            " [ 0.0702155   0.12261452  0.        ]\n",
            " [-0.28053384 -0.48988499  0.        ]\n",
            " [ 0.06653328  0.1161844   0.        ]\n",
            " [ 0.07622029  0.13310045  0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Compute vector with partial derivatives of loss function with respect to b1\n",
        "grad_b1 = relu(np.dot(W2.T, y_hat - y))\n",
        "\n",
        "# Print vector\n",
        "grad_b1\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODMFIbx-XkRy",
        "outputId": "64c783f0-1e86-4fed-aee2-c0d613ae9925"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        ],\n",
              "       [0.        ],\n",
              "       [0.17045858]])"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute matrix with partial derivatives of loss function with respect to W1\n",
        "grad_W1 = np.dot(relu(np.dot(W2.T, y_hat - y)), x.T)\n",
        "\n",
        "# Print matrix\n",
        "grad_W1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5V6ZR0tiXnX2",
        "outputId": "69bf8d28-db15-4b6d-9af3-d09ae2ad357e"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.        , 0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ],\n",
              "       [0.04261464, 0.04261464, 0.        , 0.08522929, 0.        ]])"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define alpha\n",
        "alpha = 0.03"
      ],
      "metadata": {
        "id": "hGXHYtvHXqgh"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute ReLU derivative\n",
        "relu_derivative = (z1 > 0).astype(float)  # 1 if z1 > 0, else 0\n",
        "\n",
        "# Compute grad_W1\n",
        "grad_W1 = np.dot((np.dot(W2.T, (y_hat - y)) * relu_derivative), x.T)\n",
        "\n",
        "# Update W1\n",
        "W1_new = W1 - alpha * grad_W1\n",
        "\n",
        "# Print updated W1\n",
        "print(W1_new)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcmXuj6hXt5N",
        "outputId": "e1e1035f-81ac-4fd0-8207-bd2fcb3ff057"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0.41751754  0.08918587 -0.23495225  0.28449331  0.41800106]\n",
            " [ 0.32812819  0.22872466 -0.23951958  0.41330976 -0.23924344]\n",
            " [ 0.26637602 -0.23846886 -0.37770863 -0.11399446  0.34008124]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('old value of W1:')\n",
        "print(W1)\n",
        "print()\n",
        "print('new value of W1:')\n",
        "print(W1_new)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KtEDp5axYHHv",
        "outputId": "ceda6417-74db-45a1-8740-b9395591c619"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "old value of W1:\n",
            "[[ 0.41687358  0.08854191 -0.23495225  0.28320538  0.41800106]\n",
            " [ 0.32735501  0.22795148 -0.23951958  0.4117634  -0.23924344]\n",
            " [ 0.26637602 -0.23846886 -0.37770863 -0.11399446  0.34008124]]\n",
            "\n",
            "new value of W1:\n",
            "[[ 0.41751754  0.08918587 -0.23495225  0.28449331  0.41800106]\n",
            " [ 0.32812819  0.22872466 -0.23951958  0.41330976 -0.23924344]\n",
            " [ 0.26637602 -0.23846886 -0.37770863 -0.11399446  0.34008124]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Update parameters\n",
        "W2_new = W2 - alpha * grad_W2\n",
        "b1_new = b1 - alpha * np.sum(grad_b1, axis=1, keepdims=True)\n",
        "b2_new = b2 - alpha * np.sum(grad_b2, axis=1, keepdims=True)\n",
        "\n",
        "# Print updated parameters\n",
        "print('W2_new')\n",
        "print(W2_new)\n",
        "print()\n",
        "print('b1_new')\n",
        "print(b1_new)\n",
        "print()\n",
        "print('b2_new')\n",
        "print(b2_new)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BruVq7_fYIOR",
        "outputId": "aada3d6c-2ca2-4217-e6ec-0a0cc4ad35c3"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W2_new\n",
            "[[-0.22384758 -0.43362588  0.13310965]\n",
            " [ 0.08265956  0.0775535   0.1772054 ]\n",
            " [ 0.19557112 -0.04637608 -0.1790735 ]\n",
            " [ 0.06855622 -0.02363691  0.36107434]\n",
            " [ 0.33251813 -0.3982269  -0.43959196]]\n",
            "\n",
            "b1_new\n",
            "[[ 0.09688219]\n",
            " [ 0.29239497]\n",
            " [-0.27875802]]\n",
            "\n",
            "b2_new\n",
            "[[ 0.02964508]\n",
            " [-0.36970753]\n",
            " [-0.10468778]\n",
            " [-0.35349417]\n",
            " [-0.0764456 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hmysybbrYRTz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}